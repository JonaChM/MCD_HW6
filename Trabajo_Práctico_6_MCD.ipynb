{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GItxU96mTk16"
   },
   "source": [
    "# MatemÃ¡tica para Ciencia de los Datos\n",
    "# Trabajo PrÃ¡ctico 6\n",
    "\n",
    "Profesor: Juan Luis Crespo MariÃ±o (basado en trabajo previo de Luis AlexÃ¡nder Calvo Valverde)\n",
    "\n",
    "Instituto TecnolÃ³gico de Costa Rica,\n",
    "\n",
    "Programa Ciencia de Datos\n",
    "\n",
    "---\n",
    "\n",
    "Fecha de entrega: 26 de Noviembre del 2023, a mÃ¡s tardar a las 6:00 pm.\n",
    "\n",
    "Medio de entrega: Por medio del TEC-Digital.\n",
    "\n",
    "Entregables: Un archivo jupyter ( .IPYNB ).\n",
    "\n",
    "Estudiante:\n",
    "1. **Jonathan Chavarria**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ce5WVgoNSSAf"
   },
   "source": [
    "## Ejercicio 1 (50 puntos)\n",
    "\n",
    "\n",
    "\n",
    "El algoritmo del descenso de gradiente sigue la idea de modificar el punto Ã³ptimo estimado de forma iterativa. Para una funciÃ³n en una\n",
    "variable $f\\left(x\\right)$, la estimaciÃ³n del punto Ã³ptimo en una iteraciÃ³n $i+1$ estÃ¡ dada por:\n",
    "\n",
    "\\begin{equation}\n",
    "x\\left(t+1\\right)=x\\left(t\\right)+\\alpha f'\\left(x\\left(t\\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "donde el coeficiente $\\alpha$ determina el *grado de confianza o velocidad* con la que el proceso de optimizaciÃ³n iterativa sigue\n",
    "la direcciÃ³n de la derivada. Para la optimizaciÃ³n de una funciÃ³n multivariable $f\\left(\\overrightarrow{x}\\left(t\\right)\\right)$ con $\\overrightarrow{x}\\in\\mathbb{R}^{n}$, la posiciÃ³n Ã³ptima se estima usando el vector gradiente:\n",
    "\n",
    "\\begin{equation}\n",
    "\\overrightarrow{x}\\left(t+1\\right)=\\overrightarrow{x}\\left(t\\right)+\\alpha\\nabla_{\\overrightarrow{x}}f\\left(\\overrightarrow{x}\\left(t\\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Para la funciÃ³n:\n",
    "\n",
    "\\begin{equation}\n",
    "f\\left(\\overrightarrow{x}\\right)=x^{2}-y^{2},\n",
    "\\end{equation}\n",
    "\n",
    "Implemente la funciÃ³n en python denominada:\n",
    "\n",
    "$$funcion\\_SGD \\left(tasa\\_aprendizaje, iteraciones, xy, tolerancia\\right)$$\n",
    "\n",
    "donde los parÃ¡metros corresponden a:\n",
    "\n",
    "* tasa_aprendizaje: es el $\\alpha$\n",
    "* iteraciones: es el mÃ¡ximo nÃºmero de iteraciones a ejecutar\n",
    "* xy: es el vector con los dos valores iniciales [x,y]\n",
    "* tolerancia: es el valor mÃ­nimo para un cambio entre iteraciÃ³n. Si la funciÃ³n de costo no mejora en al menos \"tolerancia\", sale del ciclo de iteraciÃ³n.\n",
    "\n",
    "**Nota:**\n",
    "1. Para iniciar la implementaciÃ³n puede utilizar el cÃ³digo en el cuaderno \"070_1_LACV_Optimizacion\".\n",
    "1. Cada iteraciÃ³n le generarÃ¡ un vector con dos valores ($\\overrightarrow{x}\\left(t+1\\right)$), por lo que para saber el valor de la funciÃ³n de pÃ©rdida en ese punto, evalÃºelo en la funciÃ³n inicial ($x^{2}-y^{2}$) para saber si aumentÃ³ o disminuyÃ³.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JvpiG31kSSAg",
    "outputId": "6f4873ff-aa77-49f9-9680-2cbf8a223413"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El minimo local ocurre en X =  4.856587433373004e-05  & Y =  59.99995143412564\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "def derivada_fx(x):\n",
    "    return 2*(x)\n",
    "\n",
    "def derivada_fy(y):\n",
    "    return -2*(y)\n",
    "\n",
    "def funcion_SGD(tasa_aprendizaje,iteraciones, cur_x , cur_y, tolerancia):\n",
    "    previous_step_size_x = 1\n",
    "    previous_step_size_y = 1\n",
    "    iters = 0 #iteration counter\n",
    "\n",
    "    while previous_step_size_x > tolerancia and previous_step_size_y > tolerancia and iters < iteraciones:\n",
    "\n",
    "        prev_x = cur_x #Store current x value in prev_x\n",
    "        cur_x = cur_x - tasa_aprendizaje * derivada_fx(prev_x) #Grad descent en x\n",
    "\n",
    "        prev_y = cur_y #Store current y value in prev_y\n",
    "        cur_y = cur_y - tasa_aprendizaje * derivada_fy(prev_x) #Grad descent en y\n",
    "\n",
    "        previous_step_size_x = abs(cur_x - prev_x) #Change in x\n",
    "        previous_step_Size_y = abs(cur_y - prev_y) #Change in y\n",
    "\n",
    "    iters = iters+1 #iteration count\n",
    "\n",
    "    if (iters % 10) == 0:\n",
    "        print(\"Iteration\",iters,\"\\nX value is\",cur_x , \"\\nY value is\" , cur_y) #Print iterations\n",
    "        print(\"previous_step_size >>> x = \", previous_step_size_x , \" or y = \" , previous_step_Size_y)\n",
    "\n",
    "\n",
    "    print(\"El minimo local ocurre en X = \", cur_x , \" & Y = \" , cur_y)\n",
    "\n",
    "valores_iniciales = [ 30 , 30 ]\n",
    "learning_rate = 0.01 # Learning rate\n",
    "precision = 0.000001 #This tells us when to stop the algorithm\n",
    "iterations = 10000 # maximum number of iterations\n",
    "\n",
    "print(funcion_SGD(learning_rate,iterations , valores_iniciales[0] , valores_iniciales[1] , precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRBmuhcjSSAh"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GoSa-9MSSAh"
   },
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "Para la funciÃ³n  $f_{1}\\left(x_{1},x_{2}\\right)=x_{1}^4 + x_{2}^4$\n",
    "\n",
    "Realice lo siguiente:\n",
    "\n",
    "1. En una celda de texto:\n",
    "\n",
    " - Calcule el vector gradiente. **(15 puntos)**\n",
    "\n",
    " - Calcule la matriz Hessiana. **(15 puntos)**\n",
    "\n",
    "2. Para el resultado obtenido en el punto anterior: **(20 puntos)**\n",
    "  - EvalÃºela en el punto $x_{1},x_{2}\\in\\left[4,4\\right]$.\n",
    "  - Luego aplique el criterio de la segunda derivada parcial Â¿quÃ© conclusiones saca para ese punto?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\text{1. CÃ¡lculo del vector gradiente y la matriz Hessiana: \\\\} $  \n",
    "\n",
    "$ \\textrm{Para la funciÃ³n } ð‘“1(ð‘¥1,ð‘¥2) = ð‘¥^{4}_{1} + ð‘¥^{4}_{2} \\\\$\n",
    "\n",
    "$ \\textrm{(a) CÃ¡lculo del gradiente: \\\\} $\n",
    "\n",
    "$ \\textrm{El gradiente } \\nabla ð‘“_{1}(ð‘¥_{1},ð‘¥_{2}) \\textrm{ se obtiene calculando las derivadas parciales de } ð‘“_{1} \\textrm{ respecto a } ð‘¥_{1}â€‹  \\textrm{ y } ð‘¥_{2} : \\\\$\n",
    "\n",
    "$\\frac{âˆ‚ð‘“_{1}}{âˆ‚ð‘¥_{1}} =  4ð‘¥^{3}_{1} \\textrm{ , } \\frac{âˆ‚ð‘“_{1}}{âˆ‚ð‘¥_2} = 4ð‘¥^{3}_{2} $\n",
    " \n",
    "$ \\textrm{Por lo tanto, el vector gradiente es: }\\\\$\n",
    "\n",
    "$ \\nabla f_{1}(x_{1} , x_{2}) = \\begin{bmatrix} 4x^{3}_{1} \\\\ 4x^{3}_{2} \\end{bmatrix} \\\\$\n",
    "\n",
    "$ \\textrm{(b) CÃ¡lculo de la matriz Hessiana: } \\\\$\n",
    "\n",
    "$ \\textrm{La matriz Hessiana } ð»(ð‘“_{1}) \\textrm{ se calcula obteniendo las segundas derivadas parciales de la funciÃ³n.} \\\\$ \n",
    "\n",
    "$ \\frac {âˆ‚^{2}f_{1}}{âˆ‚x^{2}_{1}} =  12x^{2}_{1} \\\\\n",
    "\\frac {âˆ‚^{2}f_{1}}{âˆ‚x^{2}_{2}} =  12x^{2}_{2} \\\\\n",
    "\\frac {âˆ‚^{2}f_{1}}{âˆ‚x_{1}âˆ‚x_{2}} =  0 \\\\$\n",
    "\n",
    "$\\textrm {Por lo tanto, la matriz Hessiana es:} \\\\$\n",
    "\n",
    "$ ð»(ð‘“_{1}) = \\begin{bmatrix} 12ð‘¥^{2}_{1} & 0 \\\\ 0 & 12x^{2}_{2} \\end{bmatrix} \\\\$\n",
    "\n",
    "\n",
    "$ \\textrm{2. EvaluaciÃ³n en el punto } ( ð‘¥_{1} , x_{2}) = (4,4) : \\\\$\n",
    "\n",
    "$\\textrm{ (a) Gradiente en el punto (4,4): } \\\\$\n",
    "\n",
    "$ \\nabla f(4 , 4) =  \\begin{bmatrix} 4(4)^{3} \\\\ 4(4)^{3} \\end{bmatrix} = \\begin{bmatrix} 256 \\\\ 256 \\end{bmatrix} \\\\$\n",
    "\n",
    "$\\textrm{ (b) Hessiana en el punto (4,4): } \\\\$\n",
    "\n",
    "$ \\textrm{Sustituyendo: }x_{1} = 4 , x_{2} = 4 \\textrm{ en la matriz Hessiana: } \\\\$\n",
    " \n",
    "$ ð»(ð‘“_{1}) = \\begin{bmatrix} 12(4)^{2} & 0 \\\\ 0 & 12(4)^{2} \\end{bmatrix} = \\begin{bmatrix} 192 & 0 \\\\ 0 & 192 \\end{bmatrix}\\\\$\n",
    "\n",
    "$ \\textrm{(c) AplicaciÃ³n del criterio de la segunda derivada parcial: }\\\\$\n",
    "\n",
    "$ \\textrm{ det(H) = 192*192 - 0*0 =  36,864 } \\\\$\n",
    "\n",
    "$ \\textrm{Dado que  det(H) }> 0 \\textrm{ y dado que } \\frac {âˆ‚^{2}f_{1}}{âˆ‚x^{2}_{1}} = 192 \\textrm{, es un minimo local} \\\\$\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
