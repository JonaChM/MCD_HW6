{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GItxU96mTk16"
   },
   "source": [
    "# Matemática para Ciencia de los Datos\n",
    "# Trabajo Práctico 6\n",
    "\n",
    "Profesor: Juan Luis Crespo Mariño (basado en trabajo previo de Luis Alexánder Calvo Valverde)\n",
    "\n",
    "Instituto Tecnológico de Costa Rica,\n",
    "\n",
    "Programa Ciencia de Datos\n",
    "\n",
    "---\n",
    "\n",
    "Fecha de entrega: 26 de Noviembre del 2023, a más tardar a las 6:00 pm.\n",
    "\n",
    "Medio de entrega: Por medio del TEC-Digital.\n",
    "\n",
    "Entregables: Un archivo jupyter ( .IPYNB ).\n",
    "\n",
    "Estudiante:\n",
    "1. **Jonathan Chavarria**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ce5WVgoNSSAf"
   },
   "source": [
    "## Ejercicio 1 (50 puntos)\n",
    "\n",
    "\n",
    "\n",
    "El algoritmo del descenso de gradiente sigue la idea de modificar el punto óptimo estimado de forma iterativa. Para una función en una\n",
    "variable $f\\left(x\\right)$, la estimación del punto óptimo en una iteración $i+1$ está dada por:\n",
    "\n",
    "\\begin{equation}\n",
    "x\\left(t+1\\right)=x\\left(t\\right)+\\alpha f'\\left(x\\left(t\\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "donde el coeficiente $\\alpha$ determina el *grado de confianza o velocidad* con la que el proceso de optimización iterativa sigue\n",
    "la dirección de la derivada. Para la optimización de una función multivariable $f\\left(\\overrightarrow{x}\\left(t\\right)\\right)$ con $\\overrightarrow{x}\\in\\mathbb{R}^{n}$, la posición óptima se estima usando el vector gradiente:\n",
    "\n",
    "\\begin{equation}\n",
    "\\overrightarrow{x}\\left(t+1\\right)=\\overrightarrow{x}\\left(t\\right)+\\alpha\\nabla_{\\overrightarrow{x}}f\\left(\\overrightarrow{x}\\left(t\\right)\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Para la función:\n",
    "\n",
    "\\begin{equation}\n",
    "f\\left(\\overrightarrow{x}\\right)=x^{2}-y^{2},\n",
    "\\end{equation}\n",
    "\n",
    "Implemente la función en python denominada:\n",
    "\n",
    "$$funcion\\_SGD \\left(tasa\\_aprendizaje, iteraciones, xy, tolerancia\\right)$$\n",
    "\n",
    "donde los parámetros corresponden a:\n",
    "\n",
    "* tasa_aprendizaje: es el $\\alpha$\n",
    "* iteraciones: es el máximo número de iteraciones a ejecutar\n",
    "* xy: es el vector con los dos valores iniciales [x,y]\n",
    "* tolerancia: es el valor mínimo para un cambio entre iteración. Si la función de costo no mejora en al menos \"tolerancia\", sale del ciclo de iteración.\n",
    "\n",
    "**Nota:**\n",
    "1. Para iniciar la implementación puede utilizar el código en el cuaderno \"070_1_LACV_Optimizacion\".\n",
    "1. Cada iteración le generará un vector con dos valores ($\\overrightarrow{x}\\left(t+1\\right)$), por lo que para saber el valor de la función de pérdida en ese punto, evalúelo en la función inicial ($x^{2}-y^{2}$) para saber si aumentó o disminuyó.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JvpiG31kSSAg",
    "outputId": "6f4873ff-aa77-49f9-9680-2cbf8a223413"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El minimo local ocurre en X =  4.856587433373004e-05  & Y =  59.99995143412564\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "def derivada_fx(x):\n",
    "    return 2*(x)\n",
    "\n",
    "def derivada_fy(y):\n",
    "    return -2*(y)\n",
    "\n",
    "def funcion_SGD(tasa_aprendizaje,iteraciones, cur_x , cur_y, tolerancia):\n",
    "    previous_step_size_x = 1\n",
    "    previous_step_size_y = 1\n",
    "    iters = 0 #iteration counter\n",
    "\n",
    "    while previous_step_size_x > tolerancia and previous_step_size_y > tolerancia and iters < iteraciones:\n",
    "\n",
    "        prev_x = cur_x #Store current x value in prev_x\n",
    "        cur_x = cur_x - tasa_aprendizaje * derivada_fx(prev_x) #Grad descent en x\n",
    "\n",
    "        prev_y = cur_y #Store current y value in prev_y\n",
    "        cur_y = cur_y - tasa_aprendizaje * derivada_fy(prev_x) #Grad descent en y\n",
    "\n",
    "        previous_step_size_x = abs(cur_x - prev_x) #Change in x\n",
    "        previous_step_Size_y = abs(cur_y - prev_y) #Change in y\n",
    "\n",
    "    iters = iters+1 #iteration count\n",
    "\n",
    "    if (iters % 10) == 0:\n",
    "        print(\"Iteration\",iters,\"\\nX value is\",cur_x , \"\\nY value is\" , cur_y) #Print iterations\n",
    "        print(\"previous_step_size >>> x = \", previous_step_size_x , \" or y = \" , previous_step_Size_y)\n",
    "\n",
    "\n",
    "    print(\"El minimo local ocurre en X = \", cur_x , \" & Y = \" , cur_y)\n",
    "\n",
    "valores_iniciales = [ 30 , 30 ]\n",
    "learning_rate = 0.01 # Learning rate\n",
    "precision = 0.000001 #This tells us when to stop the algorithm\n",
    "iterations = 10000 # maximum number of iterations\n",
    "\n",
    "print(funcion_SGD(learning_rate,iterations , valores_iniciales[0] , valores_iniciales[1] , precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRBmuhcjSSAh"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7GoSa-9MSSAh"
   },
   "source": [
    "## Ejercicio 2\n",
    "\n",
    "Para la función  $f_{1}\\left(x_{1},x_{2}\\right)=x_{1}^4 + x_{2}^4$\n",
    "\n",
    "Realice lo siguiente:\n",
    "\n",
    "1. En una celda de texto:\n",
    "\n",
    " - Calcule el vector gradiente. **(15 puntos)**\n",
    "\n",
    " - Calcule la matriz Hessiana. **(15 puntos)**\n",
    "\n",
    "2. Para el resultado obtenido en el punto anterior: **(20 puntos)**\n",
    "  - Evalúela en el punto $x_{1},x_{2}\\in\\left[4,4\\right]$.\n",
    "  - Luego aplique el criterio de la segunda derivada parcial ¿qué conclusiones saca para ese punto?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fNCDadfESSAh"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
